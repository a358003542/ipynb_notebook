{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch的张量\n",
    "pytorch的张量和numpy的ndarray对象相比，多了GPU计算支持和自动微分支持。而在具体使用语法上非常相似，下面不会给出过多的解释，只是给出一些代码片段，看看输出是否是你心中想的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59])\n",
      "torch.Size([60])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "t_1 = torch.arange(3*4*5)\n",
    "print(t_1)\n",
    "print(t_1.shape)\n",
    "print(t_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59]]])\n",
      "torch.Size([3, 4, 5])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "t_1 = t_1.reshape(3,4,-1)\n",
    "print(t_1)\n",
    "print(t_1.shape)\n",
    "print(t_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# zeros\n",
    "t_2 = torch.zeros(3,4,5)\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# ones\n",
    "t_3 = torch.ones(3,4,5)\n",
    "print(t_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6197, 0.4289, 0.7787, 0.1606, 0.1643],\n",
       "         [0.8005, 0.9559, 0.6820, 0.7387, 0.5123],\n",
       "         [0.9465, 0.6000, 0.5535, 0.8892, 0.7596],\n",
       "         [0.0502, 0.7657, 0.0827, 0.5335, 0.4434]],\n",
       "\n",
       "        [[0.5814, 0.0769, 0.6520, 0.7216, 0.2113],\n",
       "         [0.5906, 0.8641, 0.2657, 0.0104, 0.1978],\n",
       "         [0.4860, 0.5115, 0.7121, 0.1078, 0.9044],\n",
       "         [0.4777, 0.6741, 0.1351, 0.3047, 0.3914]],\n",
       "\n",
       "        [[0.9972, 0.1498, 0.5555, 0.7830, 0.8576],\n",
       "         [0.7460, 0.6982, 0.5741, 0.0640, 0.3117],\n",
       "         [0.8744, 0.1787, 0.3502, 0.5414, 0.4150],\n",
       "         [0.8294, 0.8559, 0.4008, 0.5101, 0.5790]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand是[0,1)的均匀分布采样\n",
    "t_4 = torch.rand(3,4,5)\n",
    "t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3690, -1.2125,  0.9713,  0.4438,  0.2695],\n",
       "         [-1.2590, -0.1723,  0.2416,  1.1429,  1.5999],\n",
       "         [-0.1632, -0.7154,  0.2331, -1.0099, -0.4292],\n",
       "         [-0.6885,  0.1371,  1.6853, -0.6001,  1.1662]],\n",
       "\n",
       "        [[-0.9101, -0.9169,  1.0831,  0.0043,  0.1835],\n",
       "         [ 1.5337,  0.6460, -0.8395, -0.1553,  0.4278],\n",
       "         [ 1.5331,  2.2837, -0.7637, -1.0780, -0.0724],\n",
       "         [-0.8201, -1.0202, -0.4884,  0.6331,  0.5497]],\n",
       "\n",
       "        [[ 0.4832,  0.2854, -0.0367,  1.6824, -1.0917],\n",
       "         [ 1.6993,  1.5096, -0.4693, -0.5930,  0.9404],\n",
       "         [-1.6893,  0.1939,  0.4922,  0.5521,  2.0050],\n",
       "         [ 0.6059,  0.9814,  1.0248, -0.1422,  0.0547]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randn是均值为0、标准差为1的标准正态分布采样\n",
    "t_5 = torch.randn(3,4,5)\n",
    "t_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元素张量\n",
    "单元素张量可以用 `item()` 来将该元素取出来，返回的是python对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "t_6 = torch.tensor([5])\n",
    "print(t_6.shape)\n",
    "print(t_6.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60.)\n",
      "60.0\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "t_7 = t_3.sum()\n",
    "print(t_7)\n",
    "print(t_7.item())\n",
    "print(type(t_7.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按元素运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,4,5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.ones(3,4,5) * 2) + torch.ones(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False,  True,  True,  True],\n",
       "         [False, False,  True,  True,  True],\n",
       "         [False, False,  True, False, False],\n",
       "         [False,  True,  True, False,  True]],\n",
       "\n",
       "        [[False, False,  True,  True,  True],\n",
       "         [ True,  True, False, False,  True],\n",
       "         [ True,  True, False, False, False],\n",
       "         [False, False, False,  True,  True]],\n",
       "\n",
       "        [[ True,  True, False,  True, False],\n",
       "         [ True,  True, False, False,  True],\n",
       "         [False,  True,  True,  True,  True],\n",
       "         [ True,  True,  True, False,  True]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_5 > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "t_2[:,:,1] = 1\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对于维度的再思考\n",
    "numpy和pytorch里面都有一个维度的概念，某些函数的运作可以指定一个dim参数，一维二维的情况还好，再多的维度试图从图形来理解并不是个好办法。下面提供了另外一种思路。\n",
    "\n",
    "一个基本的思路是：将张量按照指定维度拆分，然后将这些拆分出来的张量应用目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40, 41, 42, 43, 44],\n",
       "        [45, 46, 47, 48, 49],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [55, 56, 57, 58, 59]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60,  63,  66,  69,  72],\n",
       "        [ 75,  78,  81,  84,  87],\n",
       "        [ 90,  93,  96,  99, 102],\n",
       "        [105, 108, 111, 114, 117]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# = t_1[0]+t_1[1]+t_1[2]\n",
    "t_1.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6197, 0.4289, 0.7787, 0.1606, 0.1643],\n",
       "         [0.8005, 0.9559, 0.6820, 0.7387, 0.5123],\n",
       "         [0.9465, 0.6000, 0.5535, 0.8892, 0.7596],\n",
       "         [0.0502, 0.7657, 0.0827, 0.5335, 0.4434]],\n",
       "\n",
       "        [[0.5814, 0.0769, 0.6520, 0.7216, 0.2113],\n",
       "         [0.5906, 0.8641, 0.2657, 0.0104, 0.1978],\n",
       "         [0.4860, 0.5115, 0.7121, 0.1078, 0.9044],\n",
       "         [0.4777, 0.6741, 0.1351, 0.3047, 0.3914]],\n",
       "\n",
       "        [[0.9972, 0.1498, 0.5555, 0.7830, 0.8576],\n",
       "         [0.7460, 0.6982, 0.5741, 0.0640, 0.3117],\n",
       "         [0.8744, 0.1787, 0.3502, 0.5414, 0.4150],\n",
       "         [0.8294, 0.8559, 0.4008, 0.5101, 0.5790]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6197, 0.4289, 0.7787, 0.1606, 0.1643],\n",
       "        [0.5814, 0.0769, 0.6520, 0.7216, 0.2113],\n",
       "        [0.9972, 0.1498, 0.5555, 0.7830, 0.8576]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8005, 0.9559, 0.6820, 0.7387, 0.5123],\n",
       "        [0.5906, 0.8641, 0.2657, 0.0104, 0.1978],\n",
       "        [0.7460, 0.6982, 0.5741, 0.0640, 0.3117]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9465, 0.6000, 0.5535, 0.8892, 0.7596],\n",
       "        [0.4860, 0.5115, 0.7121, 0.1078, 0.9044],\n",
       "        [0.8744, 0.1787, 0.3502, 0.5414, 0.4150]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0502, 0.7657, 0.0827, 0.5335, 0.4434],\n",
       "        [0.4777, 0.6741, 0.1351, 0.3047, 0.3914],\n",
       "        [0.8294, 0.8559, 0.4008, 0.5101, 0.5790]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 1, 0, 2, 2],\n",
       "        [1, 1, 2, 0, 2],\n",
       "        [0, 3, 1, 0, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =argmax(t_4[:,0,:],t_4[:,1,:],t_4[:,2,:],t_4[:,3,:])\n",
    "t_4.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使是最简单的情况，因为批量处理的存在，神经网络的一个输出可能是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个维度存储的样本计数\n",
    "t_8 = torch.rand(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 3, 2, 4, 5, 4, 6, 9, 2, 1, 2, 6, 8, 6, 9, 0, 2, 5, 1, 2, 7, 0, 1, 7,\n",
       "        4, 9, 6, 8, 1, 4, 1, 0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4131, 0.8180, 0.3917, 0.3345, 0.8670, 0.5456, 0.3986, 0.9495, 0.2092,\n",
       "        0.1439, 0.4612, 0.6389, 0.4393, 0.6853, 0.1444, 0.9543, 0.0125, 0.7579,\n",
       "        0.4564, 0.6202, 0.4166, 0.9375, 0.5368, 0.4997, 0.9461, 0.4795, 0.0059,\n",
       "        0.7026, 0.6019, 0.1864, 0.8529, 0.9390])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4892, 0.4910, 0.5342, 0.2114, 0.4533, 0.8597, 0.3824, 0.1042, 0.5499,\n",
       "        0.9526, 0.0244, 0.4279, 0.0041, 0.2420, 0.7186, 0.8301, 0.1459, 0.7017,\n",
       "        0.8894, 0.8296, 0.7008, 0.2696, 0.7596, 0.1588, 0.0218, 0.8417, 0.0926,\n",
       "        0.3920, 0.9952, 0.3645, 0.9781, 0.6934])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照上面的分析，就能理解为什么 `t_8.argmax(dim=1)` 的值的含义就是一个样本的输出的最大值索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat\n",
    "cat用于张量连接，一般需要指定按照那个维度来连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "t_9 = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "print(t_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_10 = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(t_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t_9, t_10), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6., 7.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4., 3.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以理解为上面的输出按照维度=0拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t_9,t_10), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 4., 8.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拼接的情况太复杂了，参考上面的运行代码大概会有个概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 和numpy进行数据交换\n",
    "```\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "```\n",
    "\n",
    "```\n",
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存的原地修改\n",
    "用索引和切片对张量进行操作就是原地修改的，pytorch里面有些方法也是原地修改的，这里要说的主要是基于python的赋值语句：`x = x + 1` ,python的内在机制是如果变量是可变对象，那么修改之后变量对象是会重新创建的，这在这边大型张量环境下可能是个问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n",
      "2075447541616\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2075987627536\n"
     ]
    }
   ],
   "source": [
    "t_2 = t_2 + 1\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过切片语法来实现原地修改，节省内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]]])\n",
      "2075987627536\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2075987627536\n"
     ]
    }
   ],
   "source": [
    "t_2[:] = t_2 + 1\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "类似numpy，pytorch这边也有广播机制，这里也不做过多介绍了，并不鼓励这样的编码风格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "t_11 = torch.arange(3).reshape((3, 1))\n",
    "t_12 = torch.arange(2).reshape((1, 2))\n",
    "print(t_11)\n",
    "print(t_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0],\n",
       "         [1, 1],\n",
       "         [2, 2]]),\n",
       " tensor([[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.broadcast_tensors(t_11, t_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_11 + t_12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
