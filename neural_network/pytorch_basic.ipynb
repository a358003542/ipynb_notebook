{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch的张量\n",
    "pytorch的张量和numpy的ndarray对象相比，多了GPU计算支持和自动微分支持。而在具体使用语法上非常相似，下面不会给出过多的解释，只是给出一些代码片段，看看输出是否是你心中想的。然后为了增加难度，下面的例子都是基于三维数组或三维张量的，在numpy学习中虽然对维度或轴的概念有所讨论，但还有不清晰的地方，而下面的例子会增加更多关于多维张量对于不同维度下的操作的例子，来巩固关于维度的概念认识。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59])\n",
      "torch.Size([60])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "t_1 = torch.arange(3*4*5)\n",
    "print(t_1)\n",
    "print(t_1.shape)\n",
    "print(t_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59]]])\n",
      "torch.Size([3, 4, 5])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "t_1 = t_1.reshape(3,4,-1)\n",
    "print(t_1)\n",
    "print(t_1.shape)\n",
    "print(t_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# zeros\n",
    "t_2 = torch.zeros(3,4,5)\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# ones\n",
    "t_3 = torch.ones(3,4,5)\n",
    "print(t_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9591, 0.7805, 0.1832, 0.9778, 0.5701],\n",
       "         [0.5727, 0.6393, 0.6210, 0.8258, 0.7200],\n",
       "         [0.4206, 0.8784, 0.5067, 0.4370, 0.6536],\n",
       "         [0.7615, 0.1032, 0.1864, 0.4418, 0.0425]],\n",
       "\n",
       "        [[0.9602, 0.7479, 0.1072, 0.4155, 0.4677],\n",
       "         [0.2439, 0.3598, 0.9582, 0.5439, 0.0049],\n",
       "         [0.3712, 0.4989, 0.9907, 0.2396, 0.6040],\n",
       "         [0.3541, 0.1247, 0.3067, 0.0287, 0.2035]],\n",
       "\n",
       "        [[0.2149, 0.7138, 0.2733, 0.2306, 0.2903],\n",
       "         [0.4126, 0.1004, 0.0186, 0.4085, 0.2176],\n",
       "         [0.8045, 0.9717, 0.5300, 0.5964, 0.0284],\n",
       "         [0.4018, 0.5685, 0.6582, 0.9032, 0.1596]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand是[0,1)的均匀分布采样\n",
    "t_4 = torch.rand(3,4,5)\n",
    "t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.8329, -1.0569,  0.3654, -1.3916,  1.6998],\n",
       "         [-0.5710, -1.1047,  0.3352, -0.9737,  1.2224],\n",
       "         [ 0.3886,  2.7589,  0.9745, -2.1717, -0.5654],\n",
       "         [-0.2131,  0.9637, -0.9736,  0.2258,  0.5001]],\n",
       "\n",
       "        [[ 1.0111,  0.4891, -0.5739,  0.1086, -1.4090],\n",
       "         [ 0.4589,  0.6775,  0.4539, -0.7697,  0.6262],\n",
       "         [-0.7611,  0.5675,  0.2772,  1.3586, -0.3607],\n",
       "         [ 0.6609,  0.7658, -1.1333,  0.7950,  2.4345]],\n",
       "\n",
       "        [[-1.5311,  1.6685, -0.9046, -0.3233,  2.4309],\n",
       "         [ 0.9631, -1.2094, -0.3847, -0.2302,  0.0982],\n",
       "         [ 0.1748, -1.5426,  0.5219,  1.3923,  0.9860],\n",
       "         [ 0.2124, -0.6483, -0.6837,  0.0768, -0.5100]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randn是均值为0、标准差为1的标准正态分布采样\n",
    "t_5 = torch.randn(3,4,5)\n",
    "t_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元素张量\n",
    "单元素张量可以用 `item()` 来将该元素取出来，返回的是python对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "t_6 = torch.tensor([5])\n",
    "print(t_6.shape)\n",
    "print(t_6.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60.)\n",
      "60.0\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "t_7 = t_3.sum()\n",
    "print(t_7)\n",
    "print(t_7.item())\n",
    "print(type(t_7.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按元素运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,4,5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.ones(3,4,5) * 2) + torch.ones(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False,  True, False,  True],\n",
       "         [False, False,  True, False,  True],\n",
       "         [ True,  True,  True, False, False],\n",
       "         [False,  True, False,  True,  True]],\n",
       "\n",
       "        [[ True,  True, False,  True, False],\n",
       "         [ True,  True,  True, False,  True],\n",
       "         [False,  True,  True,  True, False],\n",
       "         [ True,  True, False,  True,  True]],\n",
       "\n",
       "        [[False,  True, False, False,  True],\n",
       "         [ True, False, False, False,  True],\n",
       "         [ True, False,  True,  True,  True],\n",
       "         [ True, False, False,  True, False]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_5 > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "t_2[:,:,1] = 1\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对于维度的再思考\n",
    "numpy和pytorch里面都有一个维度的概念，某些函数的运作可以指定一个dim参数，一维二维的情况还好，再多的维度试图从图形来理解并不是个好办法。下面提供了另外一种思路。\n",
    "\n",
    "一个基本的思路是：将张量按照指定维度拆分，然后将这些拆分出来的张量应用目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40, 41, 42, 43, 44],\n",
       "        [45, 46, 47, 48, 49],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [55, 56, 57, 58, 59]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60,  63,  66,  69,  72],\n",
       "        [ 75,  78,  81,  84,  87],\n",
       "        [ 90,  93,  96,  99, 102],\n",
       "        [105, 108, 111, 114, 117]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# = t_1[0]+t_1[1]+t_1[2]\n",
    "t_1.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9591, 0.7805, 0.1832, 0.9778, 0.5701],\n",
       "         [0.5727, 0.6393, 0.6210, 0.8258, 0.7200],\n",
       "         [0.4206, 0.8784, 0.5067, 0.4370, 0.6536],\n",
       "         [0.7615, 0.1032, 0.1864, 0.4418, 0.0425]],\n",
       "\n",
       "        [[0.9602, 0.7479, 0.1072, 0.4155, 0.4677],\n",
       "         [0.2439, 0.3598, 0.9582, 0.5439, 0.0049],\n",
       "         [0.3712, 0.4989, 0.9907, 0.2396, 0.6040],\n",
       "         [0.3541, 0.1247, 0.3067, 0.0287, 0.2035]],\n",
       "\n",
       "        [[0.2149, 0.7138, 0.2733, 0.2306, 0.2903],\n",
       "         [0.4126, 0.1004, 0.0186, 0.4085, 0.2176],\n",
       "         [0.8045, 0.9717, 0.5300, 0.5964, 0.0284],\n",
       "         [0.4018, 0.5685, 0.6582, 0.9032, 0.1596]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9591, 0.7805, 0.1832, 0.9778, 0.5701],\n",
       "        [0.9602, 0.7479, 0.1072, 0.4155, 0.4677],\n",
       "        [0.2149, 0.7138, 0.2733, 0.2306, 0.2903]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5727, 0.6393, 0.6210, 0.8258, 0.7200],\n",
       "        [0.2439, 0.3598, 0.9582, 0.5439, 0.0049],\n",
       "        [0.4126, 0.1004, 0.0186, 0.4085, 0.2176]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4206, 0.8784, 0.5067, 0.4370, 0.6536],\n",
       "        [0.3712, 0.4989, 0.9907, 0.2396, 0.6040],\n",
       "        [0.8045, 0.9717, 0.5300, 0.5964, 0.0284]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7615, 0.1032, 0.1864, 0.4418, 0.0425],\n",
       "        [0.3541, 0.1247, 0.3067, 0.0287, 0.2035],\n",
       "        [0.4018, 0.5685, 0.6582, 0.9032, 0.1596]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2, 1, 0, 1],\n",
       "        [0, 0, 2, 1, 2],\n",
       "        [2, 2, 3, 3, 0]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =argmax(t_4[:,0,:],t_4[:,1,:],t_4[:,2,:],t_4[:,3,:])\n",
    "t_4.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使是最简单的情况，因为批量处理的存在，神经网络的一个输出可能是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4902, 0.6946, 0.0139, 0.9022, 0.7996, 0.4757, 0.5094, 0.0212, 0.8062,\n",
      "         0.0703],\n",
      "        [0.4700, 0.8158, 0.3337, 0.0952, 0.7728, 0.0825, 0.4175, 0.6852, 0.2696,\n",
      "         0.0349],\n",
      "        [0.4462, 0.8509, 0.8168, 0.9507, 0.0070, 0.7057, 0.8904, 0.6736, 0.2686,\n",
      "         0.7722],\n",
      "        [0.8587, 0.8703, 0.4061, 0.9965, 0.6060, 0.2984, 0.2297, 0.2281, 0.7751,\n",
      "         0.1337],\n",
      "        [0.0977, 0.2662, 0.9287, 0.5681, 0.4454, 0.2427, 0.7583, 0.5844, 0.7678,\n",
      "         0.5011],\n",
      "        [0.2047, 0.9886, 0.1651, 0.9053, 0.2849, 0.0948, 0.8249, 0.4567, 0.3832,\n",
      "         0.7760],\n",
      "        [0.8068, 0.9956, 0.9873, 0.0414, 0.0709, 0.7108, 0.3767, 0.0520, 0.7823,\n",
      "         0.3459],\n",
      "        [0.4292, 0.5894, 0.9248, 0.6396, 0.3892, 0.7812, 0.2213, 0.8578, 0.3397,\n",
      "         0.6944],\n",
      "        [0.8805, 0.1064, 0.9994, 0.2305, 0.6669, 0.9244, 0.2218, 0.9122, 0.8185,\n",
      "         0.4052],\n",
      "        [0.5308, 0.8049, 0.4376, 0.8305, 0.2441, 0.3586, 0.2453, 0.7767, 0.5278,\n",
      "         0.5357],\n",
      "        [0.7981, 0.6056, 0.8114, 0.4234, 0.0173, 0.0569, 0.1885, 0.1861, 0.5019,\n",
      "         0.0961],\n",
      "        [0.9654, 0.1576, 0.6407, 0.2857, 0.2743, 0.6027, 0.5135, 0.2063, 0.1265,\n",
      "         0.7661],\n",
      "        [0.7651, 0.3459, 0.7453, 0.4556, 0.2637, 0.1230, 0.2153, 0.4593, 0.0525,\n",
      "         0.8132],\n",
      "        [0.3505, 0.9660, 0.7221, 0.8066, 0.7944, 0.1486, 0.1412, 0.7462, 0.0160,\n",
      "         0.0076],\n",
      "        [0.8650, 0.2967, 0.9861, 0.1850, 0.8829, 0.7812, 0.3241, 0.7788, 0.3831,\n",
      "         0.8687],\n",
      "        [0.8340, 0.5649, 0.3328, 0.0453, 0.0145, 0.2708, 0.4938, 0.6501, 0.2581,\n",
      "         0.8616],\n",
      "        [0.4073, 0.2100, 0.3060, 0.0302, 0.3364, 0.3449, 0.9177, 0.8072, 0.5651,\n",
      "         0.5683],\n",
      "        [0.0466, 0.4913, 0.3728, 0.4938, 0.3864, 0.2053, 0.7014, 0.4766, 0.4074,\n",
      "         0.0535],\n",
      "        [0.8018, 0.0858, 0.9378, 0.2343, 0.3022, 0.9457, 0.3221, 0.5449, 0.4896,\n",
      "         0.9765],\n",
      "        [0.2706, 0.4063, 0.4953, 0.9211, 0.0096, 0.1793, 0.4047, 0.6036, 0.8386,\n",
      "         0.6674],\n",
      "        [0.2060, 0.4101, 0.1438, 0.0222, 0.8319, 0.8631, 0.9725, 0.9242, 0.0303,\n",
      "         0.6342],\n",
      "        [0.0410, 0.4943, 0.3854, 0.4073, 0.1583, 0.7204, 0.7722, 0.7970, 0.6243,\n",
      "         0.3037],\n",
      "        [0.3252, 0.3249, 0.7077, 0.9495, 0.3162, 0.2626, 0.0829, 0.2767, 0.2459,\n",
      "         0.6576],\n",
      "        [0.3004, 0.4640, 0.5977, 0.9527, 0.0367, 0.5876, 0.6671, 0.4378, 0.4988,\n",
      "         0.3821],\n",
      "        [0.2037, 0.3447, 0.1607, 0.1770, 0.7070, 0.3559, 0.6830, 0.2651, 0.7029,\n",
      "         0.8637],\n",
      "        [0.4491, 0.6123, 0.2818, 0.2812, 0.7438, 0.5075, 0.7431, 0.3217, 0.1504,\n",
      "         0.8976],\n",
      "        [0.6657, 0.3139, 0.0314, 0.3034, 0.0538, 0.6364, 0.7934, 0.5918, 0.3305,\n",
      "         0.3835],\n",
      "        [0.9071, 0.8880, 0.9414, 0.8107, 0.2775, 0.1254, 0.7225, 0.8975, 0.4765,\n",
      "         0.4300],\n",
      "        [0.2103, 0.5418, 0.8162, 0.6453, 0.6230, 0.9472, 0.1644, 0.9851, 0.6922,\n",
      "         0.8779],\n",
      "        [0.8953, 0.3636, 0.9085, 0.9709, 0.4182, 0.5638, 0.0884, 0.7994, 0.8840,\n",
      "         0.4688],\n",
      "        [0.7761, 0.2521, 0.3534, 0.0463, 0.4542, 0.1955, 0.7700, 0.4555, 0.2950,\n",
      "         0.9978],\n",
      "        [0.7915, 0.1875, 0.2792, 0.5590, 0.8094, 0.5622, 0.2685, 0.2281, 0.1796,\n",
      "         0.4782]])\n"
     ]
    }
   ],
   "source": [
    "# 第一个维度存储的样本计数\n",
    "t_8 = torch.rand(32,10)\n",
    "print(t_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 1, 3, 3, 2, 1, 1, 2, 2, 3, 2, 0, 9, 1, 2, 9, 6, 6, 9, 3, 6, 7, 3, 3,\n",
       "        9, 9, 6, 2, 7, 3, 9, 4])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4902, 0.4700, 0.4462, 0.8587, 0.0977, 0.2047, 0.8068, 0.4292, 0.8805,\n",
       "        0.5308, 0.7981, 0.9654, 0.7651, 0.3505, 0.8650, 0.8340, 0.4073, 0.0466,\n",
       "        0.8018, 0.2706, 0.2060, 0.0410, 0.3252, 0.3004, 0.2037, 0.4491, 0.6657,\n",
       "        0.9071, 0.2103, 0.8953, 0.7761, 0.7915])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6946, 0.8158, 0.8509, 0.8703, 0.2662, 0.9886, 0.9956, 0.5894, 0.1064,\n",
       "        0.8049, 0.6056, 0.1576, 0.3459, 0.9660, 0.2967, 0.5649, 0.2100, 0.4913,\n",
       "        0.0858, 0.4063, 0.4101, 0.4943, 0.3249, 0.4640, 0.3447, 0.6123, 0.3139,\n",
       "        0.8880, 0.5418, 0.3636, 0.2521, 0.1875])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照上面的分析，就能理解为什么 `t_8.argmax(dim=1)` 的值的含义就是一个样本的输出的最大值索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat\n",
    "cat用于张量连接，一般需要指定按照那个维度来连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "t_9 = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "print(t_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_10 = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(t_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t_9, t_10), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6., 7.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4., 3.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以理解为上面的输出按照维度=0拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t_9,t_10), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 4., 8.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拼接的情况太复杂了，参考上面的运行代码大概会有个概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 和numpy进行数据交换\n",
    "```\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "```\n",
    "\n",
    "```\n",
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存的原地修改\n",
    "用索引和切片对张量进行操作就是原地修改的，pytorch里面有些方法也是原地修改的，这里要说的主要是基于python的赋值语句：`x = x + 1` ,python的内在机制是如果变量是可变对象，那么修改之后变量对象是会重新创建的，这在这边大型张量环境下可能是个问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n",
      "1681409032912\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1679798334112\n"
     ]
    }
   ],
   "source": [
    "t_2 = t_2 + 1\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过切片语法来实现原地修改，节省内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]]])\n",
      "1679798334112\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1679798334112\n"
     ]
    }
   ],
   "source": [
    "t_2[:] = t_2 + 1\n",
    "print(id(t_2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
