{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pytorch的张量\n",
    "pytorch的张量和numpy的ndarray对象相比，多了GPU计算支持和自动微分支持。而在具体使用语法上非常相似，下面不会给出过多的解释，只是给出一些代码片段，看看输出是否是你心中想的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53,\n",
      "        54, 55, 56, 57, 58, 59])\n",
      "torch.Size([60])\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "t_1 = torch.arange(3*4*5)\n",
    "print(t_1)\n",
    "print(t_1.shape)\n",
    "print(t_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0,  1,  2,  3,  4],\n",
      "         [ 5,  6,  7,  8,  9],\n",
      "         [10, 11, 12, 13, 14],\n",
      "         [15, 16, 17, 18, 19]],\n",
      "\n",
      "        [[20, 21, 22, 23, 24],\n",
      "         [25, 26, 27, 28, 29],\n",
      "         [30, 31, 32, 33, 34],\n",
      "         [35, 36, 37, 38, 39]],\n",
      "\n",
      "        [[40, 41, 42, 43, 44],\n",
      "         [45, 46, 47, 48, 49],\n",
      "         [50, 51, 52, 53, 54],\n",
      "         [55, 56, 57, 58, 59]]])\n",
      "torch.Size([3, 4, 5])\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "t_1 = t_1.reshape(3,4,-1)\n",
    "print(t_1)\n",
    "print(t_1.shape)\n",
    "print(t_1.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "# zeros\n",
    "t_2 = torch.zeros(3,4,5)\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# ones\n",
    "t_3 = torch.ones(3,4,5)\n",
    "print(t_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8113, 0.3394, 0.2720, 0.9207, 0.0957],\n",
       "         [0.4811, 0.3076, 0.9587, 0.1539, 0.9291],\n",
       "         [0.1717, 0.2102, 0.7829, 0.9354, 0.6514],\n",
       "         [0.3664, 0.8242, 0.4934, 0.1438, 0.2941]],\n",
       "\n",
       "        [[0.0441, 0.3608, 0.0146, 0.5653, 0.0251],\n",
       "         [0.9815, 0.6901, 0.4134, 0.7310, 0.7277],\n",
       "         [0.2793, 0.3744, 0.9256, 0.7317, 0.4547],\n",
       "         [0.3886, 0.7216, 0.5853, 0.1700, 0.3274]],\n",
       "\n",
       "        [[0.4683, 0.8302, 0.5207, 0.5959, 0.0076],\n",
       "         [0.8260, 0.5574, 0.2004, 0.9668, 0.6882],\n",
       "         [0.1189, 0.0598, 0.2422, 0.1047, 0.2615],\n",
       "         [0.4006, 0.1177, 0.1680, 0.2726, 0.1412]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rand是[0,1)的均匀分布采样\n",
    "t_4 = torch.rand(3,4,5)\n",
    "t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.3702, -0.2107, -0.1919, -0.8309, -0.3276],\n",
       "         [-1.9017, -0.9945, -0.9329, -0.0573, -0.4386],\n",
       "         [ 0.2649, -0.6930, -0.2420, -1.6502,  0.4475],\n",
       "         [ 0.4511, -0.6654,  1.1086, -2.5350, -0.9212]],\n",
       "\n",
       "        [[ 1.3208,  0.1201,  0.5752,  1.4812, -1.1766],\n",
       "         [-1.5987, -1.4406, -0.9006, -1.1557,  0.4850],\n",
       "         [-0.9849,  0.0805,  1.2830,  1.0878,  0.5475],\n",
       "         [ 1.1013,  0.0366, -2.4024, -1.1793, -0.7856]],\n",
       "\n",
       "        [[ 0.2563,  0.4505,  0.5686, -1.3972,  1.9129],\n",
       "         [-0.4102, -1.3756, -1.2572,  0.0318,  0.5648],\n",
       "         [ 0.6800, -1.0321,  0.5484, -1.1757, -0.4194],\n",
       "         [-0.6382,  3.1950,  0.5966,  0.0533, -1.3183]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# randn是均值为0、标准差为1的标准正态分布采样\n",
    "t_5 = torch.randn(3,4,5)\n",
    "t_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单元素张量\n",
    "单元素张量可以用 `item()` 来将该元素取出来，返回的是python对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "t_6 = torch.tensor([5])\n",
    "print(t_6.shape)\n",
    "print(t_6.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(60.)\n",
      "60.0\n",
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "t_7 = t_3.sum()\n",
    "print(t_7)\n",
    "print(t_7.item())\n",
    "print(type(t_7.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 按元素运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]],\n",
       "\n",
       "        [[2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.],\n",
       "         [2., 2., 2., 2., 2.]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(3,4,5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]],\n",
       "\n",
       "        [[3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.],\n",
       "         [3., 3., 3., 3., 3.]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.ones(3,4,5) * 2) + torch.ones(3,4,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ True, False, False, False, False],\n",
       "         [False, False, False, False, False],\n",
       "         [ True, False, False, False,  True],\n",
       "         [ True, False,  True, False, False]],\n",
       "\n",
       "        [[ True,  True,  True,  True, False],\n",
       "         [False, False, False, False,  True],\n",
       "         [False,  True,  True,  True,  True],\n",
       "         [ True,  True, False, False, False]],\n",
       "\n",
       "        [[ True,  True,  True, False,  True],\n",
       "         [False, False, False,  True,  True],\n",
       "         [ True, False,  True, False, False],\n",
       "         [False,  True,  True,  True, False]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_5 > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 索引和切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0., 0.]]])\n",
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "t_2[:,:,1] = 1\n",
    "print(t_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对于维度的再思考\n",
    "numpy和pytorch里面都有一个维度的概念，某些函数的运作可以指定一个dim参数，一维二维的情况还好，再多的维度试图从图形来理解并不是个好办法。下面提供了另外一种思路。\n",
    "\n",
    "一个基本的思路是：将张量按照指定维度拆分，然后将这些拆分出来的张量应用目标函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9],\n",
       "         [10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]],\n",
       "\n",
       "        [[20, 21, 22, 23, 24],\n",
       "         [25, 26, 27, 28, 29],\n",
       "         [30, 31, 32, 33, 34],\n",
       "         [35, 36, 37, 38, 39]],\n",
       "\n",
       "        [[40, 41, 42, 43, 44],\n",
       "         [45, 46, 47, 48, 49],\n",
       "         [50, 51, 52, 53, 54],\n",
       "         [55, 56, 57, 58, 59]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4],\n",
       "        [ 5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14],\n",
       "        [15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 21, 22, 23, 24],\n",
       "        [25, 26, 27, 28, 29],\n",
       "        [30, 31, 32, 33, 34],\n",
       "        [35, 36, 37, 38, 39]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[40, 41, 42, 43, 44],\n",
       "        [45, 46, 47, 48, 49],\n",
       "        [50, 51, 52, 53, 54],\n",
       "        [55, 56, 57, 58, 59]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_1[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 60,  63,  66,  69,  72],\n",
       "        [ 75,  78,  81,  84,  87],\n",
       "        [ 90,  93,  96,  99, 102],\n",
       "        [105, 108, 111, 114, 117]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# = t_1[0]+t_1[1]+t_1[2]\n",
    "t_1.sum(dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.8113, 0.3394, 0.2720, 0.9207, 0.0957],\n",
       "         [0.4811, 0.3076, 0.9587, 0.1539, 0.9291],\n",
       "         [0.1717, 0.2102, 0.7829, 0.9354, 0.6514],\n",
       "         [0.3664, 0.8242, 0.4934, 0.1438, 0.2941]],\n",
       "\n",
       "        [[0.0441, 0.3608, 0.0146, 0.5653, 0.0251],\n",
       "         [0.9815, 0.6901, 0.4134, 0.7310, 0.7277],\n",
       "         [0.2793, 0.3744, 0.9256, 0.7317, 0.4547],\n",
       "         [0.3886, 0.7216, 0.5853, 0.1700, 0.3274]],\n",
       "\n",
       "        [[0.4683, 0.8302, 0.5207, 0.5959, 0.0076],\n",
       "         [0.8260, 0.5574, 0.2004, 0.9668, 0.6882],\n",
       "         [0.1189, 0.0598, 0.2422, 0.1047, 0.2615],\n",
       "         [0.4006, 0.1177, 0.1680, 0.2726, 0.1412]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8113, 0.3394, 0.2720, 0.9207, 0.0957],\n",
       "        [0.0441, 0.3608, 0.0146, 0.5653, 0.0251],\n",
       "        [0.4683, 0.8302, 0.5207, 0.5959, 0.0076]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4811, 0.3076, 0.9587, 0.1539, 0.9291],\n",
       "        [0.9815, 0.6901, 0.4134, 0.7310, 0.7277],\n",
       "        [0.8260, 0.5574, 0.2004, 0.9668, 0.6882]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1717, 0.2102, 0.7829, 0.9354, 0.6514],\n",
       "        [0.2793, 0.3744, 0.9256, 0.7317, 0.4547],\n",
       "        [0.1189, 0.0598, 0.2422, 0.1047, 0.2615]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3664, 0.8242, 0.4934, 0.1438, 0.2941],\n",
       "        [0.3886, 0.7216, 0.5853, 0.1700, 0.3274],\n",
       "        [0.4006, 0.1177, 0.1680, 0.2726, 0.1412]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_4[:,3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 1, 2, 1],\n",
       "        [1, 3, 2, 2, 1],\n",
       "        [1, 0, 0, 1, 1]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =argmax(t_4[:,0,:],t_4[:,1,:],t_4[:,2,:],t_4[:,3,:])\n",
    "t_4.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即使是最简单的情况，因为批量处理的存在，神经网络的一个输出可能是这样的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一个维度存储的样本计数\n",
    "t_8 = torch.rand(32,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 6, 7, 1, 2, 9, 2, 4, 4, 7, 7, 8, 9, 1, 2, 2, 0, 7, 7, 6, 9, 0, 8, 8,\n",
       "        5, 4, 6, 4, 7, 7, 3, 9])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2131, 0.6024, 0.3581, 0.6646, 0.4971, 0.8565, 0.1846, 0.7581, 0.4805,\n",
       "        0.9173, 0.8863, 0.6624, 0.4382, 0.8482, 0.6016, 0.7282, 0.9545, 0.0485,\n",
       "        0.3723, 0.4181, 0.0277, 0.8681, 0.4732, 0.3896, 0.8014, 0.8284, 0.2809,\n",
       "        0.8951, 0.8465, 0.4928, 0.5625, 0.2149])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0336, 0.5129, 0.6291, 0.9438, 0.7302, 0.8149, 0.3268, 0.5691, 0.7621,\n",
       "        0.9023, 0.0142, 0.6073, 0.6022, 0.9698, 0.5560, 0.5511, 0.0410, 0.4358,\n",
       "        0.8208, 0.9180, 0.7810, 0.7887, 0.9228, 0.3740, 0.4470, 0.5264, 0.2442,\n",
       "        0.5503, 0.0455, 0.1221, 0.3526, 0.6658])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_8[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照上面的分析，就能理解为什么 `t_8.argmax(dim=1)` 的值的含义就是一个样本的输出的最大值索引。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cat\n",
    "cat用于张量连接，一般需要指定按照那个维度来连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.]])\n"
     ]
    }
   ],
   "source": [
    "t_9 = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
    "print(t_9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 1., 4., 3.],\n",
      "        [1., 2., 3., 4.],\n",
      "        [4., 3., 2., 1.]])\n"
     ]
    }
   ],
   "source": [
    "t_10 = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
    "print(t_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.],\n",
       "        [ 8.,  9., 10., 11.],\n",
       "        [ 2.,  1.,  4.,  3.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t_9, t_10), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 1., 2., 3.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6., 7.])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4., 3.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以理解为上面的输出按照维度=0拼接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
       "        [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
       "        [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((t_9,t_10), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 4., 8.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 5., 9.])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_9[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 1., 4.])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_10[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "拼接的情况太复杂了，参考上面的运行代码大概会有个概念。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 和numpy进行数据交换\n",
    "```\n",
    "np_array = np.array(data)\n",
    "x_np = torch.from_numpy(np_array)\n",
    "```\n",
    "\n",
    "```\n",
    "t = torch.ones(5)\n",
    "n = t.numpy()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 节省内存的原地修改\n",
    "用索引和切片对张量进行操作就是原地修改的，pytorch里面有些方法也是原地修改的，这里要说的主要是基于python的赋值语句：`x = x + 1` ,python的内在机制是如果变量是可变对象，那么修改之后变量对象是会重新创建的，这在这边大型张量环境下可能是个问题了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0., 0.]]])\n",
      "2175214939712\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2175214942752\n"
     ]
    }
   ],
   "source": [
    "t_2 = t_2 + 1\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以通过切片语法来实现原地修改，节省内存开销。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.],\n",
      "         [1., 2., 1., 1., 1.]]])\n",
      "2175214942752\n"
     ]
    }
   ],
   "source": [
    "print(t_2)\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2175214942752\n"
     ]
    }
   ],
   "source": [
    "t_2[:] = t_2 + 1\n",
    "print(id(t_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 广播机制\n",
    "类似numpy，pytorch这边也有广播机制，这里也不做过多介绍了，并不鼓励这样的编码风格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0],\n",
      "        [1],\n",
      "        [2]])\n",
      "tensor([[0, 1]])\n"
     ]
    }
   ],
   "source": [
    "t_11 = torch.arange(3).reshape((3, 1))\n",
    "t_12 = torch.arange(2).reshape((1, 2))\n",
    "print(t_11)\n",
    "print(t_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0],\n",
       "         [1, 1],\n",
       "         [2, 2]]),\n",
       " tensor([[0, 1],\n",
       "         [0, 1],\n",
       "         [0, 1]]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.broadcast_tensors(t_11, t_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1],\n",
       "        [1, 2],\n",
       "        [2, 3]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_11 + t_12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
